{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "809de71f-f76b-4693-8456-e6ef73920c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def read_json_to_df(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# 读取各个文件到独立的DataFrame\n",
    "df_train = read_json_to_df('E:/Desktop/Text/Task2/train.json')\n",
    "df_test = read_json_to_df('E:/Desktop/Text/Task2/test.json')\n",
    "df_valid = read_json_to_df('E:/Desktop/Text/Task2/valid.json')\n",
    "\n",
    "# 将数字转换为字符串（对tokens和tags进行处理）\n",
    "df_train['tokens'] = df_train['tokens'].apply(lambda x: [str(i) for i in x])\n",
    "df_train['tags'] = df_train['tags'].apply(lambda x: [str(i) for i in x])\n",
    "df_valid['tokens'] = df_valid['tokens'].apply(lambda x: [str(i) for i in x])\n",
    "df_valid['tags'] = df_valid['tags'].apply(lambda x: [str(i) for i in x])\n",
    "df_test['tokens'] = df_test['tokens'].apply(lambda x: [str(i) for i in x])\n",
    "df_test['tags'] = df_test['tags'].apply(lambda x: [str(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63405e51-769c-4b91-ac9e-820054f52b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 提取tokens和tags\n",
    "train_sentences = df_train['tokens'].tolist()\n",
    "train_tags = df_train['tags'].tolist()\n",
    "valid_sentences = df_valid['tokens'].tolist()\n",
    "valid_tags = df_valid['tags'].tolist()\n",
    "test_sentences = df_test['tokens'].tolist()\n",
    "test_tags = df_test['tags'].tolist()\n",
    "\n",
    "# 创建tokenizer\n",
    "tokenizer = Tokenizer(num_words=5120, oov_token='UNK')\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "\n",
    "# 文本转序列\n",
    "train_seq = tokenizer.texts_to_sequences(train_sentences)\n",
    "valid_seq = tokenizer.texts_to_sequences(valid_sentences)\n",
    "test_seq = tokenizer.texts_to_sequences(test_sentences)\n",
    "\n",
    "# 序列填充\n",
    "max_len = 46\n",
    "train_seq_padded = pad_sequences(train_seq, maxlen=max_len, padding='post')\n",
    "valid_seq_padded = pad_sequences(valid_seq, maxlen=max_len, padding='post')\n",
    "test_seq_padded = pad_sequences(test_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "# 标签编码\n",
    "tag_tokenizer = Tokenizer()\n",
    "tag_tokenizer.fit_on_texts(train_tags)\n",
    "\n",
    "# 标签转序列\n",
    "train_tag_seq = tag_tokenizer.texts_to_sequences(train_tags)\n",
    "valid_tag_seq = tag_tokenizer.texts_to_sequences(valid_tags)\n",
    "test_tag_seq = tag_tokenizer.texts_to_sequences(test_tags)\n",
    "\n",
    "# 标签填充\n",
    "train_tag_seq_padded = pad_sequences(train_tag_seq, maxlen=max_len, padding='post')\n",
    "valid_tag_seq_padded = pad_sequences(valid_tag_seq, maxlen=max_len, padding='post')\n",
    "test_tag_seq_padded = pad_sequences(test_tag_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "# 标签one-hot编码\n",
    "num_tags = len(tag_tokenizer.word_index) + 1\n",
    "train_tags_encoded = to_categorical(train_tag_seq_padded, num_classes=num_tags)\n",
    "valid_tags_encoded = to_categorical(valid_tag_seq_padded, num_classes=num_tags)\n",
    "test_tags_encoded = to_categorical(test_tag_seq_padded, num_classes=num_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ee307c3-d86b-4e8b-b511-b9842f9004c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - accuracy: 0.8747 - loss: 0.4986 - val_accuracy: 0.9527 - val_loss: 0.1307\n",
      "Epoch 2/10\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.9600 - loss: 0.1145 - val_accuracy: 0.9719 - val_loss: 0.0912\n",
      "Epoch 3/10\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - accuracy: 0.9818 - loss: 0.0642 - val_accuracy: 0.9769 - val_loss: 0.0710\n",
      "Epoch 4/10\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.9885 - loss: 0.0377 - val_accuracy: 0.9772 - val_loss: 0.0756\n",
      "Epoch 5/10\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.9904 - loss: 0.0294 - val_accuracy: 0.9779 - val_loss: 0.0778\n",
      "Epoch 6/10\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.9931 - loss: 0.0220 - val_accuracy: 0.9772 - val_loss: 0.0756\n",
      "Epoch 7/10\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.9941 - loss: 0.0181 - val_accuracy: 0.9777 - val_loss: 0.0906\n",
      "Epoch 8/10\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.9954 - loss: 0.0151 - val_accuracy: 0.9771 - val_loss: 0.0920\n",
      "Epoch 9/10\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.9963 - loss: 0.0123 - val_accuracy: 0.9779 - val_loss: 0.0952\n",
      "Epoch 10/10\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.9969 - loss: 0.0098 - val_accuracy: 0.9772 - val_loss: 0.0968\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9787 - loss: 0.0859\n",
      "Test loss: 0.09126169979572296\n",
      "Test accuracy: 0.9778454899787903\n"
     ]
    }
   ],
   "source": [
    "# 构建BiLSTM模型\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Bidirectional, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5120, output_dim=128, input_length=max_len))\n",
    "model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(TimeDistributed(Dense(num_tags, activation='softmax')))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# 训练和评估模型\n",
    "history = model.fit(train_seq_padded, train_tags_encoded, batch_size=32, epochs=10, validation_data=(valid_seq_padded, valid_tags_encoded))\n",
    "\n",
    "loss, accuracy = model.evaluate(test_seq_padded, test_tags_encoded)\n",
    "print(\"Test loss:\", loss)\n",
    "print(\"Test accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4fe5f50-1bc9-46a6-b93f-afa645f9a9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 寻找最优序列长度和词汇表\n",
    "# from collections import Counter\n",
    "\n",
    "# def analyze_data(df):\n",
    "#     # 计算每个句子的长度\n",
    "#     df['sentence_length'] = df['tokens'].apply(len)\n",
    "\n",
    "#     # 统计所有词的出现次数\n",
    "#     word_counts = Counter(word for tokens_list in df['tokens'] for word in tokens_list)\n",
    "\n",
    "#     return df['sentence_length'].describe(), word_counts\n",
    "\n",
    "# # 分析训练数据\n",
    "# sentence_length_desc, word_counts = analyze_data(df_train)\n",
    "\n",
    "# # 打印句子长度的描述性统计信息\n",
    "# print(\"Sentence length statistics:\\n\", sentence_length_desc)\n",
    "\n",
    "# # 确定合适的序列长度：选择覆盖95%数据的长度\n",
    "# max_length = int(np.percentile(df_train['sentence_length'], 95))\n",
    "# print(\"Recommended max sequence length (95 percentile):\", max_length)\n",
    "\n",
    "# # 确定词汇表大小：选择覆盖95%词频的词汇表大小\n",
    "# cumulative_coverage = 0\n",
    "# total_frequency = sum(word_counts.values())\n",
    "# sorted_words = sorted(word_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "# vocab_size = 0\n",
    "# for i, (word, freq) in enumerate(sorted_words):\n",
    "#     cumulative_coverage += freq / total_frequency\n",
    "#     if cumulative_coverage >= 0.95:\n",
    "#         vocab_size = i + 1\n",
    "#         break\n",
    "\n",
    "# print(f\"Recommended vocabulary size to cover 95% of all word occurrences: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f3c7c2-098c-467c-bafc-0e8d57d18626",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_analytics",
   "language": "python",
   "name": "text_analytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
