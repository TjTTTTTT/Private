{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cad4e17-ce73-46d5-897d-3a8fc7c8410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filling missing values\n",
    "# import pandas as pd\n",
    "# # Load the dataset\n",
    "# penguins_data = pd.read_csv('E:/Desktop/AI/penguins.csv')\n",
    "\n",
    "# # Define the function to fill missing values using different decimal places for different numerical data\n",
    "# def fill_with_group_stats(df, group_cols, fill_cols, decimal_places):\n",
    "#     # Fill numerical data using specified decimal places\n",
    "#     for col in fill_cols['mean']:\n",
    "#         if col in decimal_places:\n",
    "#             # Round to the specified number of decimal places\n",
    "#             df[col] = df.groupby(group_cols)[col].transform(lambda x: x.fillna(round(x.mean(), decimal_places[col])))\n",
    "#         else:\n",
    "#             # By default, do not round, use the raw mean\n",
    "#             df[col] = df.groupby(group_cols)[col].transform(lambda x: x.fillna(x.mean()))\n",
    "#     # Fill categorical data using the mode\n",
    "#     for col in fill_cols['mode']:\n",
    "#         df[col] = df.groupby(group_cols)[col].transform(lambda x: x.fillna(x.mode()[0] if not x.mode().empty else None))\n",
    "#     return df\n",
    "\n",
    "# # Apply the fill function\n",
    "# group_columns = ['species', 'island', 'year']\n",
    "# fill_columns = {\n",
    "#     'mean': ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g'], \n",
    "#     'mode': ['sex']\n",
    "# }\n",
    "# decimal_places = {\n",
    "#     'bill_length_mm': 1,\n",
    "#     'bill_depth_mm': 1,\n",
    "#     'flipper_length_mm': 0,\n",
    "#     'body_mass_g': 0\n",
    "# }\n",
    "\n",
    "# penguins_data = fill_with_group_stats(penguins_data, group_columns, fill_columns, decimal_places)\n",
    "\n",
    "# # Save the updated dataset to a CSV file\n",
    "# penguins_data.to_csv('E:/Desktop/AI/updated_penguins.csv', index=False)\n",
    "\n",
    "# # Check for any remaining missing values\n",
    "# updated_missing_data = penguins_data.isnull().sum()\n",
    "# print(updated_missing_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4eb7b5-97d6-4869-8dd1-50cca05c3505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check for outliers, few are found\n",
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Load the dataset\n",
    "# penguins_data = pd.read_csv('E:/Desktop/AI/updated_penguins.csv')\n",
    "\n",
    "# # Define a function to check for outliers\n",
    "# def plot_boxplots(df, group_cols, numeric_cols):\n",
    "#     # Generate a boxplot for each numerical field\n",
    "#     for col in numeric_cols:\n",
    "#         plt.figure(figsize=(12, 6))\n",
    "#         sns.boxplot(data=df, x='species', y=col, hue='island')\n",
    "#         plt.title(f'Boxplot of {col} grouped by Species and Island')\n",
    "#         plt.xlabel('Species')\n",
    "#         plt.ylabel(col)\n",
    "#         plt.legend(title='Island')\n",
    "#         plt.show()\n",
    "\n",
    "# # Call the function to generate boxplots\n",
    "# numeric_columns = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n",
    "# plot_boxplots(penguins_data, ['species', 'island'], numeric_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45f4bb2-c3db-4056-bd64-2822d6bfe264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Load the dataset\n",
    "# penguins = pd.read_csv('E:/Desktop/AI/updated_penguins.csv')\n",
    "\n",
    "# # Group by species and island, and calculate the mean for selected numerical columns\n",
    "# grouped_means = penguins.groupby(['species', 'island'])[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']].mean()\n",
    "# # grouped_means.to_csv('E:/Desktop/AI/grouped_means.csv')\n",
    "# latex_table = grouped_means.to_latex()\n",
    "# print(latex_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292ac79b-abc3-4a3e-a848-01d4cb7ba1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # Using KMeans clustering to obtain cluster labels\n",
    "# import pandas as pd\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.decomposition import PCA\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import silhouette_score\n",
    "# # Load the dataset\n",
    "# penguins_data = pd.read_csv('E:/Desktop/AI/updated_penguins.csv')\n",
    "\n",
    "# # Select features\n",
    "# features = penguins_data[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']]\n",
    "\n",
    "# # Standardize the data\n",
    "# scaler = StandardScaler()\n",
    "# features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# # Apply K-Means clustering\n",
    "# kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "# kmeans.fit(features_scaled)\n",
    "\n",
    "# # Obtain cluster labels\n",
    "# penguins_data['cluster'] = kmeans.labels_\n",
    "\n",
    "# # # Calculate the silhouette score\n",
    "# # score = silhouette_score(features_scaled, kmeans.labels_)\n",
    "# # print('Silhouette Score: %.2f' % score)\n",
    "\n",
    "# # # PCA reduction to 2D for visualization\n",
    "# # pca = PCA(n_components=2)\n",
    "# # principal_components = pca.fit_transform(features_scaled)\n",
    "\n",
    "# # # Visualization\n",
    "# # plt.figure(figsize=(8, 6))\n",
    "# # plt.scatter(principal_components[:, 0], principal_components[:, 1], c=penguins_data['cluster'], cmap='viridis', marker='o')\n",
    "# # plt.title('PCA of Penguin Dataset with K-Means Clustering')\n",
    "# # plt.xlabel('Principal Component 1')\n",
    "# # plt.ylabel('Principal Component 2')\n",
    "# # plt.colorbar(label='Cluster')\n",
    "# # plt.show()\n",
    "\n",
    "# # # penguins_data.to_csv('E:/Desktop/AI/Kmeans_penguins.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed7073e-f565-4b4f-ba1a-953889c6bb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Splitting the file into training, validation, and test sets with proportions of 70% (training set):15% (validation set):15% (test set)\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # Load the dataset\n",
    "# data = pd.read_csv('E:/Desktop/AI/KmeansRevise_penguins.csv')\n",
    "\n",
    "# # Assume 'cluster' is the target category label modified by K-Means\n",
    "# features = data[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']]\n",
    "# labels = data['cluster']\n",
    "\n",
    "# # Split the dataset into training set and temporary set (includes validation set and test set)\n",
    "# X_train, X_temp, y_train, y_temp = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# # Split the temporary set into validation set and test set\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a098f97-f7a7-4bc5-85b6-01e6ee8e8bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Finding the optimal hyperparameters\n",
    "# # Assume X_train, y_train, X_val, y_val are already defined\n",
    "# # Define hyperparameter grids for SVM and Random Forest\n",
    "\n",
    "# param_grid_svm = {\n",
    "#     # 'C': [1, 1.5, 1.7,2],\n",
    "#     'C': [1.6, 1.7, 1.8, 1.9, 2],\n",
    "#     'kernel': ['linear', 'rbf', 'poly']\n",
    "# }\n",
    "\n",
    "# param_grid_rf = {\n",
    "#     # 'n_estimators': [85, 100, 175, 50, 70, 65],\n",
    "#     'n_estimators': [75, 85, 90],\n",
    "#     'max_features': ['auto', 'sqrt', 'log2'],\n",
    "#     'max_depth': [None, 2, 4]\n",
    "# }\n",
    "\n",
    "# # Create a GridSearchCV object for the SVM model\n",
    "# # grid_search_svm = GridSearchCV(SVC(), param_grid_svm, refit=True, verbose=0)\n",
    "# # grid_search_svm.fit(X_train, y_train)\n",
    "# # print(\"Best parameters for SVM:\", grid_search_svm.best_params_)\n",
    "\n",
    "# # Create a GridSearchCV object for the Random Forest model\n",
    "# grid_search_rf = GridSearchCV(Random ForestClassifier(random_state=42), param_grid_rf, refit=True, verbose=0)\n",
    "# grid_search_rf.fit(X_train, y_train)\n",
    "# print(\"Best parameters for Random Forest:\", grid_search_rf.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d117128d-51cb-4927-9148-1b2e20c24063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import cross_val_score, train_test_split\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.metrics include accuracy_score, f1_score\n",
    "# from sklearn.dummy import DummyClassifier  # Import DummyClassifier\n",
    "\n",
    "# # Load the dataset\n",
    "# data = pd.read_csv('E:/Desktop/AI/KmeansRevise_penguins.csv')\n",
    "\n",
    "# # Prepare features and labels\n",
    "# features = data[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']]\n",
    "# labels = data['cluster']\n",
    "# X_train_val, X_test, y_train_val, y_test = train_test_split(features, labels, test_size=0.15, random_state=42)\n",
    "\n",
    "# # Create models\n",
    "# svm_model = make_pipeline(StandardScaler(), SVC(kernel='linear', C=1.7))\n",
    "# rf_model = RandomForestClassifier(n_estimators=85, max_features='sqrt', max_depth=None, random_state=42)\n",
    "# baseline_model = DummyClassifier(strategy='most_frequent')  # Create baseline model\n",
    "\n",
    "# # Perform 5-fold cross-validation\n",
    "# svm_scores = cross_val_score(svm_model, X_train_val, y_train_val, cv=5)\n",
    "# rf_scores = cross_val_score(rf_model, X_train_val, y_train_val, cv=5)\n",
    "\n",
    "# # Print cross-validation results\n",
    "# print(\"Average SVM Cross-validated score:\", svm_scores.mean())\n",
    "# print(\"Average Random Forest Cross-validated score:\", rf_scores.mean())\n",
    "\n",
    "# # Retrain models on the entire training validation set\n",
    "# svm_model.fit(X_train_val, y_train_val)\n",
    "# rf_model.fit(X_train_val, y_train_val)\n",
    "# baseline_model.fit(X_train_val, y_train_val)  # Train baseline model\n",
    "\n",
    "# # Final evaluation on the test set\n",
    "# svm_test_predictions = svm_model.predict(X_test)\n",
    "# rf_test_predictions = rf_model.predict(X_test)\n",
    "# baseline_test_predictions = baseline_model.predict(X_test)  # Predict baseline model\n",
    "\n",
    "# # Calculate accuracy and F1 score\n",
    "# svm_test_accuracy = accuracy_score(y_test, svm_test_predictions)\n",
    "# rf_test_accuracy = accuracy_score(y_test, rf_test_predictions)\n",
    "# baseline_test_accuracy = accuracy_score(y_test, baseline_test_predictions)  # Baseline model accuracy\n",
    "\n",
    "# svm_test_f1 = f1_score(y_test, svm_test_predictions, average='macro')\n",
    "# rf_test_f1 = f1_score(y_test, rf_test_predictions, average='macro')\n",
    "# baseline_test_f1 = f1_score(y_test, baseline_test_predictions, average='macro')  # Baseline model F1 score\n",
    "\n",
    "# # Print test set accuracy and F1 scores\n",
    "# print(\"SVM Test accuracy:\", svm_test_accuracy)\n",
    "# print(\"Random Forest Test accuracy:\", rf_test_accuracy)\n",
    "# print(\"Baseline Model Test accuracy:\", baseline_test_accuracy)  # Print baseline model accuracy\n",
    "\n",
    "# print(\"SVM Test F1 Score:\", svm_test_f1)\n",
    "# print(\"Random Forest Test F1 Score:\", rf_test_f1)\n",
    "# print(\"Baseline Model Test F1 Score:\", baseline_test_f1)  # Print baseline model F1 score\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_analytics",
   "language": "python",
   "name": "text_analytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
