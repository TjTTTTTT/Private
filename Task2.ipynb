{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87a987b7-25f1-41c8-b691-ae470d683e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def read_json_to_df(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Read each file into a separate DataFrame\n",
    "df_train = read_json_to_df('E:/Desktop/Text/Task2/train.json')\n",
    "df_test = read_json_to_df('E:/Desktop/Text/Task2/test.json')\n",
    "df_valid = read_json_to_df('E:/Desktop/Text/Task2/valid.json')\n",
    "\n",
    "# Convert numbers to strings (processing tokens and tags)\n",
    "df_train['tokens'] = df_train['tokens'].apply(lambda x: [str(i) for i in x])\n",
    "df_train['tags'] = df_train['tags'].apply(lambda x: [str(i) for i in x])\n",
    "df_valid['tokens'] = df_valid['tokens'].apply(lambda x: [str(i) for i in x])\n",
    "df_valid['tags'] = df_valid['tags'].apply(lambda x: [str(i) for i in x])\n",
    "df_test['tokens'] = df_test['tokens'].apply(lambda x: [str(i) for i in x])\n",
    "df_test['tags'] = df_test['tags'].apply(lambda x: [str(i) for i in x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14f9dce5-24d7-4290-aef8-4ead8ea6abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract tokens and tags\n",
    "train_sentences = df_train['tokens'].tolist()\n",
    "train_tags = df_train['tags'].tolist()\n",
    "valid_sentences = df_valid['tokens'].tolist()\n",
    "valid_tags = df_valid['tags'].tolist()\n",
    "test_sentences = df_test['tokens'].tolist()\n",
    "test_tags = df_test['tags'].tolist()\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = Tokenizer(num_words=5120, oov_token='UNK')\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "\n",
    "# Convert text to sequences\n",
    "train_seq = tokenizer.texts_to_sequences(train_sentences)\n",
    "valid_seq = tokenizer.texts_to_sequences(valid_sentences)\n",
    "test_seq = tokenizer.texts_to_sequences(test_sentences)\n",
    "\n",
    "# Pad sequences\n",
    "max_len = 46\n",
    "train_seq_padded = pad_sequences(train_seq, maxlen=max_len, padding='post')\n",
    "valid_seq_padded = pad_sequences(valid_seq, maxlen=max_len, padding='post')\n",
    "test_seq_padded = pad_sequences(test_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "# Encode tags\n",
    "tag_tokenizer = Tokenizer()\n",
    "tag_tokenizer.fit_on_texts(train_tags)\n",
    "\n",
    "# Convert tags to sequences\n",
    "train_tag_seq = tag_tokenizer.texts_to_sequences(train_tags)\n",
    "valid_tag_seq = tag_tokenizer.texts_to_sequences(valid_tags)\n",
    "test_tag_seq = tag_tokenizer.texts_to_sequences(test_tags)\n",
    "\n",
    "# Pad tag sequences\n",
    "train_tag_seq_padded = pad_sequences(train_tag_seq, maxlen=max_len, padding='post')\n",
    "valid_tag_seq_padded = pad_sequences(valid_tag_seq, maxlen=max_len, padding='post')\n",
    "test_tag_seq_padded = pad_sequences(test_tag_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "# One-hot encode tags\n",
    "num_tags = len(tag_tokenizer.word_index) + 1\n",
    "train_tags_encoded = to_categorical(train_tag_seq_padded, num_classes=num_tags)\n",
    "valid_tags_encoded = to_categorical(valid_tag_seq_padded, num_classes=num_tags)\n",
    "test_tags_encoded = to_categorical(test_tag_seq_padded, num_classes=num_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7a465b7-4902-4efc-92bf-6d41c72e7950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build a BiLSTM model\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Bidirectional, Dropout\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=5120, output_dim=128, input_length=max_len))\n",
    "# model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(TimeDistributed(Dense(num_tags, activation='softmax')))\n",
    "\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # model.summary()\n",
    "\n",
    "# # Train and evaluate the model\n",
    "# history = model.fit(train_seq_padded, train_tags_encoded, batch_size=32, epochs=10, validation_data=(valid_seq_padded, valid_tags_encoded))\n",
    "\n",
    "# loss, accuracy = model.evaluate(test_seq_padded, test_tags_encoded)\n",
    "# print(\"Test loss:\", loss)\n",
    "# print(\"Test accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95c24e87-ca81-4e84-9f1d-b809b11cde96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 47ms/step - accuracy: 0.8797 - f1_score: 0.8097 - loss: 0.5120 - val_accuracy: 0.9521 - val_f1_score: 0.9576 - val_loss: 0.1389\n",
      "Epoch 2/10\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 41ms/step - accuracy: 0.9601 - f1_score: 0.9644 - loss: 0.1160 - val_accuracy: 0.9720 - val_f1_score: 0.9716 - val_loss: 0.0946\n",
      "Epoch 3/10\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step - accuracy: 0.9819 - f1_score: 0.9812 - loss: 0.0637 - val_accuracy: 0.9755 - val_f1_score: 0.9757 - val_loss: 0.0750\n",
      "Epoch 4/10\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 41ms/step - accuracy: 0.9878 - f1_score: 0.9875 - loss: 0.0400 - val_accuracy: 0.9747 - val_f1_score: 0.9750 - val_loss: 0.0762\n",
      "Epoch 5/10\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step - accuracy: 0.9902 - f1_score: 0.9900 - loss: 0.0316 - val_accuracy: 0.9771 - val_f1_score: 0.9772 - val_loss: 0.0709\n",
      "Epoch 6/10\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step - accuracy: 0.9927 - f1_score: 0.9926 - loss: 0.0231 - val_accuracy: 0.9771 - val_f1_score: 0.9772 - val_loss: 0.0830\n",
      "Epoch 7/10\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 41ms/step - accuracy: 0.9942 - f1_score: 0.9943 - loss: 0.0183 - val_accuracy: 0.9765 - val_f1_score: 0.9768 - val_loss: 0.0805\n",
      "Epoch 8/10\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step - accuracy: 0.9956 - f1_score: 0.9955 - loss: 0.0144 - val_accuracy: 0.9771 - val_f1_score: 0.9772 - val_loss: 0.0922\n",
      "Epoch 9/10\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step - accuracy: 0.9963 - f1_score: 0.9963 - loss: 0.0127 - val_accuracy: 0.9760 - val_f1_score: 0.9763 - val_loss: 0.0993\n",
      "Epoch 10/10\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step - accuracy: 0.9968 - f1_score: 0.9968 - loss: 0.0110 - val_accuracy: 0.9764 - val_f1_score: 0.9766 - val_loss: 0.0986\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9781 - f1_score: 0.9783 - loss: 0.0873\n",
      "Test loss: 0.09288658201694489\n",
      "Test accuracy: 0.977200984954834\n",
      "Test F1 Score: 0.9774160981178284\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Bidirectional, Dropout\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a custom F1 Score metric\n",
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.precision = tf.keras.metrics.Precision()\n",
    "        self.recall = tf.keras.metrics.Recall()\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.round(y_pred)  # Apply threshold to convert probabilities to binary values\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        p = self.precision.result()\n",
    "        r = self.recall.result()\n",
    "        return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()\n",
    "\n",
    "# Build a BiLSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5120, output_dim=128, input_length=max_len))\n",
    "model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(TimeDistributed(Dense(num_tags, activation='softmax')))\n",
    "\n",
    "# Compile the model with custom F1 Score metric\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', F1Score()])\n",
    "\n",
    "# Display model summary\n",
    "# model.summary()\n",
    "\n",
    "# Train and evaluate the model\n",
    "history = model.fit(train_seq_padded, train_tags_encoded, batch_size=32, epochs=10, validation_data=(valid_seq_padded, valid_tags_encoded))\n",
    "\n",
    "loss, accuracy, f1_score = model.evaluate(test_seq_padded, test_tags_encoded)\n",
    "print(\"Test loss:\", loss)\n",
    "print(\"Test accuracy:\", accuracy)\n",
    "print(\"Test F1 Score:\", f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1c2db48-e19f-4184-a835-8c23f815ed68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence length statistics:\n",
      " count    5228.000000\n",
      "mean       20.910865\n",
      "std        14.487604\n",
      "min         1.000000\n",
      "25%        11.000000\n",
      "50%        18.000000\n",
      "75%        27.000000\n",
      "max       133.000000\n",
      "Name: sentence_length, dtype: float64\n",
      "Recommended max sequence length (95 percentile): 46\n",
      "Recommended vocabulary size to cover 95% of all word occurrences: 5120\n"
     ]
    }
   ],
   "source": [
    "# Find the optimal sequence length and vocabulary size\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_data(df):\n",
    "    # Calculate the length of each sentence\n",
    "    df['sentence_length'] = df['tokens'].apply(len)\n",
    "\n",
    "    # Count the occurrence of each word\n",
    "    word_counts = Counter(word for tokens_list in df['tokens'] for word in tokens_list)\n",
    "\n",
    "    return df['sentence_length'].describe(), word_counts\n",
    "\n",
    "# Analyze training data\n",
    "sentence_length_desc, word_counts = analyze_data(df_train)\n",
    "\n",
    "# Print descriptive statistics for sentence lengths\n",
    "print(\"Sentence length statistics:\\n\", sentence_length_desc)\n",
    "\n",
    "# Determine the appropriate sequence length: choose the length that covers 95% of the data\n",
    "max_length = int(np.percentile(df_train['sentence_length'], 95))\n",
    "print(\"Recommended max sequence length (95 percentile):\", max_length)\n",
    "\n",
    "# Determine vocabulary size: choose the vocabulary size that covers 95% of word frequencies\n",
    "cumulative_coverage = 0\n",
    "total_frequency = sum(word_counts.values())\n",
    "sorted_words = sorted(word_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "vocab_size = 0\n",
    "for i, (word, freq) in enumerate(sorted_words):\n",
    "    cumulative_coverage += freq / total_frequency\n",
    "    if cumulative_coverage >= 0.95:\n",
    "        vocab_size = i + 1\n",
    "        break\n",
    "\n",
    "print(f\"Recommended vocabulary size to cover 95% of all word occurrences: {vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438e9b94-9863-495e-97b8-32094fb6ca26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_analytics",
   "language": "python",
   "name": "text_analytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
