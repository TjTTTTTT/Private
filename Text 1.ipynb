{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "610eacd7-5b0e-4dea-9d54-c83f6e5b73ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Read Parquet files\n",
    "train_dataframe = pd.read_parquet(r'E:\\Desktop\\Text\\Task1\\train.parquet')\n",
    "val_dataframe = pd.read_parquet(r'E:\\Desktop\\Text\\Task1\\validation.parquet')\n",
    "test_dataframe = pd.read_parquet(r'E:\\Desktop\\Text\\Task1\\test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4a4f425-79fc-428a-a7b9-b67fa6b903ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing and feature extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download nltk resources\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "# Custom text cleaning and preprocessing class\n",
    "class TextPreprocessor(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        # Remove user tags and URLs\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        text = re.sub(r'www\\S+', '', text)\n",
    "        # Remove HTML tags and special characters\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove stopwords and lemmatize\n",
    "        text = ' '.join([self.lemmatizer.lemmatize(word) for word in text.split() if word not in self.stop_words])\n",
    "        return text\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [self.clean_text(text) for text in X]\n",
    "\n",
    "\n",
    "# Create data preprocessing pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('text_preprocessor', TextPreprocessor()),\n",
    "    ('tfidf_vectorizer', TfidfVectorizer())\n",
    "])\n",
    "\n",
    "\n",
    "train_texts = train_dataframe['text']\n",
    "val_texts = val_dataframe['text']\n",
    "test_texts = test_dataframe['text']\n",
    "\n",
    "# Apply preprocessing\n",
    "train_features = pipeline.fit_transform(train_texts)\n",
    "val_features = pipeline.transform(val_texts)\n",
    "test_features = pipeline.transform(test_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0a3c552-7925-4498-8294-24d10a3d426d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6903589021815623\n",
      "\n",
      "Some examples of misclassified texts:\n",
      "\n",
      "Original Text: @user #shocking loss of talented young man#prayers#pray for his family\n",
      "Processed Text: shocking loss talented young manprayerspray family\n",
      "Predicted Label: 2, Actual Label: 1\n",
      "\n",
      "Original Text: @user Really??? I've had to hang up!\n",
      "Processed Text: really ive hang\n",
      "Predicted Label: 1, Actual Label: 3\n",
      "\n",
      "Original Text: yukwon no video do zico the world is shaking\n",
      "Processed Text: yukwon video zico world shaking\n",
      "Predicted Label: 0, Actual Label: 3\n",
      "\n",
      "Original Text: IK to PMLN: 'Darling, I will haunt you in your nightmares, dressed like a dream.' BEST THING EVER. #GameOverNawaz\n",
      "Processed Text: ik pmln darling haunt nightmare dressed like dream best thing ever gameovernawaz\n",
      "Predicted Label: 0, Actual Label: 3\n",
      "\n",
      "Original Text: @user Improve on the makeup dear to avoid reduction in viewership...\n",
      "Processed Text: improve makeup dear avoid reduction viewership\n",
      "Predicted Label: 0, Actual Label: 2\n",
      "\n",
      "Original Text: @user Couldn't have delivered without you @user - #inspiring #partnership #women\n",
      "Processed Text: couldnt delivered without inspiring partnership woman\n",
      "Predicted Label: 0, Actual Label: 1\n",
      "\n",
      "Original Text: @user I don't know my mouth was burning the whole time\n",
      "Processed Text: dont know mouth burning whole time\n",
      "Predicted Label: 3, Actual Label: 1\n",
      "\n",
      "Original Text: Of course I've got a horrible cold and am breaking out 2 days before grad üëçüèºüëçüèºüëçüèºüëçüèºüëçüèº\n",
      "Processed Text: course ive got horrible cold breaking day grad\n",
      "Predicted Label: 0, Actual Label: 3\n",
      "\n",
      "Original Text: @user @user if I catch you making tea with water boiled up to 100 degrees, there will be dire consequences\n",
      "Processed Text: catch making tea water boiled degree dire consequence\n",
      "Predicted Label: 3, Actual Label: 0\n",
      "\n",
      "Original Text: Just added a copy of CANDIDE to my local Little Free Library &amp; a guy grabbed it literally 5 seconds later #optimism #BestOfAllPossibleWorlds\n",
      "Processed Text: added copy candide local little free library amp guy grabbed literally second later optimism bestofallpossibleworlds\n",
      "Predicted Label: 0, Actual Label: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Retrieve labels from DataFrame\n",
    "y_train = train_dataframe['label']\n",
    "y_val = val_dataframe['label']\n",
    "y_test = test_dataframe['label']\n",
    "\n",
    "# Merge training and validation sets\n",
    "X_train_full = sp.vstack([train_features, val_features])\n",
    "y_train_full = pd.concat([y_train, y_val])\n",
    "\n",
    "\n",
    "\n",
    "# # Set the grid of parameters to adjust\n",
    "# param_grid = {\n",
    "#     # 'C': [0.1, 1, 10,100],  # Inverse of regularization strength    \n",
    "#     # 'C': [0.5,0.75, 1, 1.25,1.5,1.75,2],  # Inverse of regularization strength\n",
    "#     'C': [1, 1.25, 1.3, 1.5, 1.6, 1.75, 2],  # Inverse of regularization strength    \n",
    "#     # 'gamma': ['scale', 'auto'],  # Kernel coefficient (for non-linear kernel)\n",
    "#     'kernel': ['linear', 'rbf']  # Type of kernel\n",
    "# }\n",
    "\n",
    "# # Create SVM model\n",
    "# svm_model = SVC(random_state=42)\n",
    "\n",
    "# # Create a grid search object, using cross-validation to evaluate each parameter setting\n",
    "# grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring='accuracy', verbose=0)\n",
    "\n",
    "# # Perform grid search using training data\n",
    "# grid_search.fit(train_features, y_train)\n",
    "\n",
    "# # Output the best parameters and corresponding performance\n",
    "# print(\"Best parameters:\", grid_search.best_params_)\n",
    "# print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "\n",
    "\n",
    "# Create SVM model\n",
    "svm_model = SVC(C = 1.25, kernel='linear', random_state=42)\n",
    "\n",
    "# Train the model\n",
    "svm_model.fit(X_train_full, y_train_full)\n",
    "\n",
    "\n",
    "# Predict on the test set\n",
    "test_predictions = svm_model.predict(test_features)\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print()\n",
    "\n",
    "# Identify misclassified samples\n",
    "misclassified_indices = np.where(test_predictions != y_test)[0]\n",
    "misclassified_samples = test_dataframe.iloc[misclassified_indices]\n",
    "misclassified_texts = misclassified_samples['text']\n",
    "misclassified_labels = misclassified_samples['label']\n",
    "\n",
    "# Create an instance of the text preprocessor\n",
    "text_preprocessor = TextPreprocessor()\n",
    "\n",
    "# Output the original and preprocessed texts\n",
    "print(\"Some examples of misclassified texts:\")\n",
    "print()\n",
    "for index in misclassified_indices[:10]:  # Display the first ten misclassified samples\n",
    "    original_text = misclassified_texts.iloc[index]\n",
    "    processed_text = text_preprocessor.clean_text(original_text)\n",
    "    print(f\"Original Text: {original_text}\")\n",
    "    print(f\"Processed Text: {processed_text}\")\n",
    "    print(f\"Predicted Label: {test_predictions[index]}, Actual Label: {y_test.iloc[index]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdda784b-5395-42dd-a82a-ac2ce258f712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prediction accuracy without preprocessing\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# train_dataframe = pd.read_parquet(r'E:\\Desktop\\Text\\Task1\\train.parquet')\n",
    "# val_dataframe = pd.read_parquet(r'E:\\Desktop\\Text\\Task1\\validation.parquet')\n",
    "# test_dataframe = pd.read_parquet(r'E:\\Desktop\\Text\\Task1\\test.parquet')\n",
    "\n",
    "# # Create instance of TF-IDF Vectorizer\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# import scipy.sparse as sp\n",
    "\n",
    "# # Set up TF-IDF vectorizer (without preprocessing)\n",
    "# no_prep_tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# # Apply TF-IDF vectorization\n",
    "# train_features_no_prep = no_prep_tfidf_vectorizer.fit_transform(train_dataframe['text'])\n",
    "# val_features_no_prep = no_prep_tfidf_vectorizer.transform(val_dataframe['text'])\n",
    "# test_features_no_prep = no_prep_tfidf_vectorizer.transform(test_dataframe['text'])\n",
    "\n",
    "# # Merge training and validation sets\n",
    "# X_train_full_no_prep = sp.vstack([train_features_no_prep, val_features_no_prep])\n",
    "# y_train_full = pd.concat([train_dataframe['label'], val_dataframe['label']])\n",
    "\n",
    "# # Create and train SVM model\n",
    "# svm_model_no_prep = SVC(C=1.25, kernel='linear', random_state=42)\n",
    "# svm_model_no_prep.fit(X_train_full_no_prep, y_train_full)\n",
    "\n",
    "# # Predict on the test set\n",
    "# test_predictions_no_prep = svm_model_no_prep.predict(test_features_no_prep)\n",
    "# test_accuracy_no_prep = accuracy_score(test_dataframe['label'], test_predictions_no_prep)\n",
    "# print(f\"Test Accuracy without preprocessing: {test_accuracy_no_prep}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d209dfc-cc4e-499e-8e3f-1e3a5fd1519b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_analytics",
   "language": "python",
   "name": "text_analytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
